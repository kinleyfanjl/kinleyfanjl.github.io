<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Machine Learning,Bayesian Inference," />










<meta name="description" content="Nonparametric Bayesian modelThere are many cases, which require a presuming parametric specifications of the probability distributions, such as observations from Normal distribution or Quening model w">
<meta name="keywords" content="Machine Learning,Bayesian Inference">
<meta property="og:type" content="article">
<meta property="og:title" content="Clustering by Nonparametric Bayes">
<meta property="og:url" content="http://yoursite.com/2018/07/29/haha/index.html">
<meta property="og:site_name" content="The Statistical Smurf">
<meta property="og:description" content="Nonparametric Bayesian modelThere are many cases, which require a presuming parametric specifications of the probability distributions, such as observations from Normal distribution or Quening model w">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2018/07/29/haha/20180729g01.png">
<meta property="og:image" content="http://yoursite.com/2018/07/29/haha/20180729g01.png">
<meta property="og:image" content="http://yoursite.com/2018/07/29/haha/nonparabayes.png">
<meta property="og:image" content="http://yoursite.com/2018/07/29/haha/20180729g02.png">
<meta property="og:updated_time" content="2018-07-29T15:04:54.105Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Clustering by Nonparametric Bayes">
<meta name="twitter:description" content="Nonparametric Bayesian modelThere are many cases, which require a presuming parametric specifications of the probability distributions, such as observations from Normal distribution or Quening model w">
<meta name="twitter:image" content="http://yoursite.com/2018/07/29/haha/20180729g01.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/07/29/haha/"/>





  <title>Clustering by Nonparametric Bayes | The Statistical Smurf</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">The Statistical Smurf</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            Schedule
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/29/haha/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kinleyfanjl">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="The Statistical Smurf">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Clustering by Nonparametric Bayes</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-29T15:20:23+08:00">
                2018-07-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Original/" itemprop="url" rel="index">
                    <span itemprop="name">Original</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Nonparametric-Bayesian-model"><a href="#Nonparametric-Bayesian-model" class="headerlink" title="Nonparametric Bayesian model"></a>Nonparametric Bayesian model</h2><p>There are many cases, which require a presuming parametric specifications of the probability distributions, such as observations from Normal distribution or Quening model with Poisson distribution, and so on. But this subjective assumption may cause misspecification of model, which will result in bad prediction of severe lack of accuracy. To avoid such cases, nonparametric methods are better, which can allow more flexible and robust specification. “Learning from data” can help reduce many risk.<br><img src="/2018/07/29/haha/20180729g01.png" title="A typical clustering task"></p>
<a id="more"></a>
<p>Indeed, nonparametric bayesian model should also have a bayesian-version style, which should contain prior, posterior, complex likelihood with kernel due to its complex expression and clever sampling methods needing expensive computation.</p>
<p>In machine learning, clustering is one big topic. There are many clustering methods. Clustering problem has an unsupervised problem setting, which means there is no available labels for the data, people have to use the intrisic structure of data to assign data points into different clusters.</p>
<p>To figure out this kind of problem, one way is to use a standard clustering algorithm like K-means or Gaussian mixture modeling. Both methods are similar, because they regard distances between data points as an important criterion for clustering, they cluster all points in a certain area together. But there is a problem that both methods need a fixed number of clusters, which should be setted by people subjectively. Although this problem can be solved by cross-validation in some extent, we now try a method, which doesn’t need the prefixed total number of clusters. By resorting to nonparametric bayes, we allow the number of clusters grow as more data comes in instead of fixing the number of cluster to be discoverd.</p>
<h2 id="CRP-process"><a href="#CRP-process" class="headerlink" title="CRP process"></a>CRP process</h2><p>CRP process is a proper generative model without fixed number of clusters. Consider a problem that we need one way to assign friend to different tables. We describe this process in a rigorous way:</p>
<ul>
<li>A chinese restaurant has an infinite number of tables:</li>
<li>first customer sits at the first table</li>
<li>m-th subsequent customer sits at a table drawn from the following distribution:<br>$$P(previously\ occupied\ table\ i|\mathcal{F}_{m-1}) \propto n_i$$<br>$$P(the\ next\ unoccupied\ table|\mathcal{F}_{m-1}) \propto \alpha$$<br>where $n_i$ is the number of customer currently at table i and where $\mathcal{F}_{m-1}$ denotes the state of the restaurant after m-1 customers have been seated.</li>
</ul>
<p>This means, the (n+1)st person sits down at a new table with probability $\frac{\alpha}{n+\alpha}$, and at table k with probability $\frac{n_k}{n+\alpha}$, where $n_k$ is the number of people currently sitting at table k, and $\alpha$ is a dispersion hyper-parameter.</p>
<p>And with the clustering assignments, we can further assume that for kth cluster there is parameter $\phi_k$, we assume that $\phi_k\sim G_0$. Thus the generative model for data $X$ is $X|\phi_k \sim F_k, \phi_k\sim G_0$. This model can generate data with overlapping clusters. In this case, K-means and Gaussian mixture are not good enough, because from scatter plot people cannot define a correct number of clusters, because points are not seperatable in the plot. </p>
<p>The R-code for CRP process is following:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">####Chinese Restaurant Process####</span><br><span class="line">crprnd = function(alpha, n)&#123;</span><br><span class="line">  </span><br><span class="line">  m = rep(0,n);</span><br><span class="line">  partition = rep(0, n);</span><br><span class="line">  partition_bin = matrix(rep(0,n*n),n,n);</span><br><span class="line">  </span><br><span class="line">  # Initialization</span><br><span class="line">  partition[1] = 1</span><br><span class="line">  partition_bin[1,1] = 1</span><br><span class="line">  m[1] = 1</span><br><span class="line">  K = 1</span><br><span class="line">  # Iterations</span><br><span class="line">  for(i in 2:n)&#123;</span><br><span class="line">    # Compute the probability of joining an existing cluster or a new one</span><br><span class="line">    proba = c(m[1:K], alpha)/(alpha+i-1)</span><br><span class="line">    # Sample from a discrete distribution w.p. proba</span><br><span class="line">    u = runif(1)</span><br><span class="line">    partition[i] = which(u&lt;=cumsum(proba))[1]</span><br><span class="line">    partition_bin[i,partition[i]] = 1</span><br><span class="line">    # Increment the size of the cluster</span><br><span class="line">    m[partition[i]] = m[partition[i]] + 1</span><br><span class="line">    # Increment the number of clusters if new</span><br><span class="line">    if(sum(m&gt;0) &gt; K) K = K + 1;     </span><br><span class="line">  &#125;</span><br><span class="line">  return(list(class = partition,class_size = m, K = K, class_decrp = partition_bin))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>This function will generate n samples from CRP process. It will return total number of clusters K and individual class vector “class”.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crprnd(alpha = 1.0, n = 20)</span><br></pre></td></tr></table></figure></p>
<h2 id="Generate-Data-amp-Prior-Assumption-amp-Create-Sampler"><a href="#Generate-Data-amp-Prior-Assumption-amp-Create-Sampler" class="headerlink" title="Generate Data &amp; Prior Assumption &amp; Create Sampler"></a>Generate Data &amp; Prior Assumption &amp; Create Sampler</h2><p>In this section, we will try the nonparametric bayesian clustering in a simple case, where data are generated from a Normal distribution with share constant variance. And $X_i$ is a p-dimension vector. The generative procedure is:</p>
<span>$$\phi_k \sim \mathcal{N}_p(\mu_0,\Sigma_0)\\
z_i\sim CRP(\alpha)\\
X_i|\phi_k,z_i = k,\Sigma \sim \mathcal{N}_p(\phi_k, \Sigma)$$</span><!-- Has MathJax -->
<h3 id="Generate-Data"><a href="#Generate-Data" class="headerlink" title="Generate Data"></a>Generate Data</h3><p>Here is the code for generating experimental dataset, we will generate 100 data points with 5 latent clusters. And we set <span>$$\mu_0= (4,7), \Sigma_0 = \begin{bmatrix}
4 &amp; 1 \\
1 &amp; 8 
\end{bmatrix},\ \alpha = 1.0,\ N=100, \Sigma = I_2\times 0.5$$</span><!-- Has MathJax --> the generation procedure and simple visual is following: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">library(mvtnorm)</span><br><span class="line">library(MASS)</span><br><span class="line">library(bayesm)</span><br><span class="line">library(ggplot2)</span><br><span class="line">mu0 = c(4,7)</span><br><span class="line">sigma0 = matrix(c(4,1,1,8),2,2,byrow=T)</span><br><span class="line">set.seed(1237)</span><br><span class="line">cluster = crprnd(alpha =1 , 100)</span><br><span class="line">Z = cluster$class</span><br><span class="line">K = cluster$K</span><br><span class="line">mu = mvrnorm(n = K,mu = mu0, Sigma = sigma0)</span><br><span class="line">sig = diag(rep(1,2))*0.5</span><br><span class="line">generateData = function(nk, muk, sigmak)&#123;</span><br><span class="line">    testdata = NULL</span><br><span class="line">    for(i in 1:length(nk))&#123;</span><br><span class="line">        t = mvrnorm(nk[i], mu = muk[i,],Sigma = sigmak)</span><br><span class="line">        testdata = rbind(testdata, t)</span><br><span class="line">    &#125;</span><br><span class="line">    testdata = cbind(testdata, rep(names(nk),nk))</span><br><span class="line">    return(testdata)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">nk = table(Z)</span><br><span class="line">testdata = generateData(nk, mu, sig)</span><br><span class="line">testdata = apply(testdata,2,as.numeric)</span><br><span class="line">testdata = data.frame(testdata)</span><br><span class="line">g = ggplot(testdata , aes(x = X1, y = X2, color = factor(X3)))</span><br><span class="line">g+  geom_point()</span><br></pre></td></tr></table></figure>
<p><img src="20180729g01.png" alt=""><br>From this graph we can see that, such dataset has obvious overlapping clusters, and these clusters can be detected easily.</p>
<h3 id="Derivation-of-Sampler"><a href="#Derivation-of-Sampler" class="headerlink" title="Derivation of Sampler"></a>Derivation of Sampler</h3><p>With this generative model, we can derive the posterior distribution of parameters with interest in. In clustering task, we want the distribution $z_i|X_i,\Sigma,\mu_0,\Sigma_0, \ \ i = 1,…,N,\ z_i \in \mathbb{N^+}$, and to get this posterior, which doesn’t has a explicit expression. So we need to resort to Gibbs sampling method, which also need us to derive the full conditional distribution or respective kernel part. Because for parameters z we can only derive its kernel part, so we can only use <strong>Hybrid Gibbs Sampling</strong>.</p>
<p><em>Algorithm of Hybrid Gibbs</em>:</p>
<ol>
<li><p>Initialization. We set $$\mu_0 = \bar{X},\Sigma_0=\frac{1}{n}XX^{T}$$ and initialize   $z_{i},\phi_{k},i\in{1,..,N},k\in{1,..,K}$ with CRP and  multivarite normal.</p>
</li>
<li><p>For the s-th echo:</p>
<p>For i from 1 to N:</p>
<ul>
<li><p>We use a proposal distribution $z^*\sim \mathcal{N}<em>{I(j\in[0.5,100])}(z^{(s)}</em>{i}, 3)$, which is due to we want cluster index is positive integer and 100 points at most have 100 clusters;</p>
</li>
<li><p>Sample $z_i$ by <em>Metroplis-Hasting</em>. Calculate the ratio $\frac{f(z^*|\cdot)}{f(z^s|\cdot)}$, where $$z_i|\phi_{\{k\}},X_,z_{-i},\Sigma_0,\mu_0\propto \sum_{k=1}^{K}n_kp(x_i|\phi_k)\mathbb{1}_{\{z_i = k\}}+ \alpha f(x_i|\mu_0,\Sigma_0)1_{\{z_i=K+1\}}$$ where $n_k$ is the number of points in cluster k.</p>
</li>
</ul>
<p>For k from 1 to K:</p>
<ul>
<li>Sample $\phi_k$ from the full conditional distribution. According to the posterior update, we have:<br>$$\phi_k|X_{i},Z_{i},\mu_0,\Sigma_0 \sim \mathcal{N}_p\big((\Sigma_0^{-1}+\Sigma^{-1})^{-1}(\Sigma^{-1}\bar{X}_k+\Sigma_0\mu_0),(\Sigma_0^{-1}+\Sigma^{-1})^{-1}\big)$$</li>
</ul>
</li>
<li><p>If $s&gt;C$, C is the echo when this sampler is stationary, we will collect samples of $(z_1,…,z_N)$.</p>
</li>
<li><p>Stop sampling until s reach S.</p>
</li>
</ol>
<p>This model is also named Dirichlet Process Gaussian Mixture Model(DPGMM) or Infinite Gaussian Mixture Model.</p>
<p>Code for calculating the loglikelihood ratio and Hybird Gibbs are following:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">#####likelihood function for Gibbs#####</span><br><span class="line">likeli = function(zi,x_i,n_vec,alpha,Kplus,mu, mu0, sigma0, sig)&#123;</span><br><span class="line">    p = length(x_i)</span><br><span class="line">    if(zi &gt; Kplus)&#123;</span><br><span class="line">        return(alpha*dmvnorm(x_i, mean = mu0, sigma = sigma0+sig))</span><br><span class="line">    &#125;</span><br><span class="line">    else return(n_vec[zi]*dmvnorm(x_i,mean = mu[zi,], sigma = sig))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mu0 = colMeans(testdata[,1:2])</span><br><span class="line">sigma0 = cov(testdata[,1:2])</span><br><span class="line">plot(testdata[,1],testdata[,2], pch = 20)</span><br><span class="line"></span><br><span class="line">####### Start the Simulation!!!!######</span><br><span class="line">alpha = 1; </span><br><span class="line">R = 10000; N = 100; p = 2</span><br><span class="line">sig = diag(rep(1,p))*0.5</span><br><span class="line">mean_vec = matrix(0)</span><br><span class="line">cluster = rep(0, N)</span><br><span class="line">####Initialization: clusters, means#####</span><br><span class="line">cluster = crprnd(alpha, N)</span><br><span class="line">K = cluster$K</span><br><span class="line">mu = mvrnorm(100, mu0, sigma0)</span><br><span class="line">Z = cluster$class</span><br><span class="line">mean_vec1 = tapply(testdata[,1],Z, mean)</span><br><span class="line">mean_vec2 = tapply(testdata[,2],Z, mean)</span><br><span class="line">mean_vec = cbind(mean_vec1, mean_vec2)</span><br><span class="line">n_vec = table(Z); K = length(n_vec)</span><br><span class="line">X = testdata[,1:2]</span><br><span class="line"></span><br><span class="line">######updating by Hybrid Gibbs#######</span><br><span class="line">sigma_update = solve(solve(sig) + solve(sigma0))</span><br><span class="line">invsigma0 = solve(sigma0)</span><br><span class="line">Z_post = NULL</span><br><span class="line"></span><br><span class="line">for(r in 1:R)&#123;</span><br><span class="line">    if(r %% 50 == 0)</span><br><span class="line">        print(paste0(&quot;this process is still alive!! Be patient! Echo = &quot;,r))</span><br><span class="line">    for( i in 1:N)&#123;</span><br><span class="line">        s = 1; S = 5</span><br><span class="line">        z = NULL</span><br><span class="line">        z.s = Z[i]</span><br><span class="line">        </span><br><span class="line">        for(s in 1:S)&#123;</span><br><span class="line">            ####choose proper proposal distribution####</span><br><span class="line">            z.star = rtrun(2, 3, 0.5, 100)</span><br><span class="line">            z.star = round(z.star)</span><br><span class="line">            ###Computer the rejection probability</span><br><span class="line">            log.r = log(likeli(z.star,X[i,],n_vec,alpha,K,mu, mu0, sigma0, sig)) - </span><br><span class="line">                log(likeli(z.s,X[i,],n_vec,alpha,K,mu, mu0, sigma0, sig))</span><br><span class="line">            </span><br><span class="line">            if (log(runif(1))&lt; log.r) &#123;z.s = z.star&#125;</span><br><span class="line">            z = c(z,z.s)</span><br><span class="line">        &#125;</span><br><span class="line">        Z[i] = z[S]</span><br><span class="line">    &#125;</span><br><span class="line">    ####update vec mu and vec n#######</span><br><span class="line">    for(k in 1:K)&#123;</span><br><span class="line">        mu_update = sigma_update%*%(solve(sig)%*%mean_vec[k,] + invsigma0 %*% mu0)</span><br><span class="line">        mu[k,] =  mvrnorm(1 ,mu = mu_update, Sigma = sigma_update)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    n_vec = table(Z) ; K = length(n_vec)</span><br><span class="line">    mean_vec1 = tapply(testdata[,2],Z, mean)</span><br><span class="line">    mean_vec2 = tapply(testdata[,3],Z, mean)</span><br><span class="line">    mean_vec = cbind(mean_vec1,mean_vec2)</span><br><span class="line">    if(r%%100 == 0)</span><br><span class="line">        paste0(&apos;the process is still alive, r=&apos;,r)</span><br><span class="line">    if(r &gt; 7000)&#123;</span><br><span class="line">        Z_post = rbind(Z_post,Z)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">####get the posterior mode####</span><br><span class="line">getcluster = function(x)&#123;</span><br><span class="line">    return(which.max(table(x)))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">####mean####</span><br><span class="line">classes = round(apply(Z_post,2,mean))</span><br><span class="line"></span><br><span class="line">finaldata = cbind(X,classes) </span><br><span class="line">####Visualization####</span><br><span class="line">ggplot(finaldata, aes(x=X1, y=X2, shape = factor(classes),color=factor(classes))) + geom_point()</span><br></pre></td></tr></table></figure></p>
<p>(After about 15mins..) The final visualization is: </p>
<p><img src="nonparabayes.png" alt=""><br>From this graph we can see that this method can catch some overlapping clusters and allow more flexibility. Although we have only 4 clusters here, but each cluster seem reasonable. A better performance may be got by other additional tools, such as sample $z_i$ with a certain gap and keep sampler working for a longer time.</p>
<p>##Kmeans method from Frequentists##<br>Kmeans method is a clustering method based on distances between data points, and the procedure is resorting to EM algorithm. In this procedure, people regard labels as missing data, and use EM algorithm to assign data points in the cluster which makes data has largest likelihood, and then update the new cluster parameters by sample means. Code is following:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kmclass = kmeans(testdata, 5)</span><br><span class="line">final = cbind(testdata[,1:2], kmclass$cluster)</span><br><span class="line">names(final) = c(&apos;x1&apos;,&apos;x2&apos;,&apos;c&apos;)</span><br><span class="line">g = ggplot(final, aes(x= x1, y= x2, color= factor(c)))</span><br><span class="line">g+ geom_point()</span><br></pre></td></tr></table></figure></p>
<p><img src="20180729g02.png" alt=""></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>From the results of both methods, we found that nonparametric bayes can handle more flexible dataset, which has complex intrinsic structure or correlation. But Kmeans-like frequentists’ methods mostly consider that under fixed groups how can people assign these data points properly. And clustering by nonparametric bayes don’t need to choose the number of clusters, which will give a smart guess when people don’t know anything about datasets in hand. And the model can be improved with cluster-specified covariance matrix. Even more complex case is that we are interested in all parameters, we can create a bigger Gibbs sampler which includes every full conditional distributions, but the computation will be very expensive. Because i-th label’s likelihood is based on current labels of other data, there is not a trivial parallel computation strategy for this problem.</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          
            <a href="/tags/Bayesian-Inference/" rel="tag"># Bayesian Inference</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/07/27/tran4/" rel="next" title="数据科学家需要了解的5种聚类算法">
                <i class="fa fa-chevron-left"></i> 数据科学家需要了解的5种聚类算法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Kinleyfanjl</p>
              <p class="site-description motion-element" itemprop="description">Ph.D in Statistics</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Nonparametric-Bayesian-model"><span class="nav-number">1.</span> <span class="nav-text">Nonparametric Bayesian model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CRP-process"><span class="nav-number">2.</span> <span class="nav-text">CRP process</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Generate-Data-amp-Prior-Assumption-amp-Create-Sampler"><span class="nav-number">3.</span> <span class="nav-text">Generate Data &amp; Prior Assumption &amp; Create Sampler</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Generate-Data"><span class="nav-number">3.1.</span> <span class="nav-text">Generate Data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Derivation-of-Sampler"><span class="nav-number">3.2.</span> <span class="nav-text">Derivation of Sampler</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">4.</span> <span class="nav-text">Conclusion</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kinleyfanjl</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
